{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import random\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_ID = 0\n",
    "DEVICE = torch.device('cuda:%d' % DEVICE_ID)\n",
    "torch.cuda.set_device(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Для запуска без GPU раскомментировать и закоментировать код выше\n",
    "#DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100500)\n",
    "\n",
    "def data2image(data):\n",
    "    res = np.transpose(np.reshape(data ,(3, 32,32)), (1,2,0))\n",
    "    return PIL.Image.fromarray(np.uint8(res))\n",
    "\n",
    "def imshow(img):\n",
    "    if isinstance(img, torch.Tensor): img = img.numpy().astype('uint8')\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    \n",
    "def prediction2classes(output_var):\n",
    "    _, predicted = torch.max(output_var.data, 1)\n",
    "    predicted.squeeze_()\n",
    "    classes = predicted.tolist()\n",
    "    return classes\n",
    "\n",
    "def make_solution_pytorch(net, input_tensor, a_batch_size):\n",
    "    res = []\n",
    "    net = net.eval()\n",
    "    cur_pos = 0\n",
    "    while cur_pos <= len(input_tensor):\n",
    "        outputs = net(input_tensor[cur_pos:cur_pos+a_batch_size])\n",
    "        res += prediction2classes(outputs)\n",
    "        cur_pos += a_batch_size\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, input_path, is_train=True, transform=None):\n",
    "                        \n",
    "        data = np.load(input_path)\n",
    "        if is_train: \n",
    "            self.Y, self.X = np.hsplit(data, [1]) \n",
    "            self.Y = [item[0] for item in self.Y]\n",
    "        else: \n",
    "            self.X = data\n",
    "            self.Y = None\n",
    "            \n",
    "        self.X = self.X.reshape((self.X.shape[0], 3, 32, 32))\n",
    "        self.X = self.X.transpose((0, 2, 3, 1)) #приводим к виду (N, H, W, C)\n",
    "        self.X = [Image.fromarray(img) for img in self.X]\n",
    "                \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = self.X[idx]\n",
    "\n",
    "        if self.transform: sample = self.transform(sample)\n",
    "\n",
    "        if self.Y is None: return sample\n",
    "        else: return (sample, self.Y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Надо поменять пути на свои до файлов с kaggle\n",
    "#DATA_PATH  = '/home/a.murashev/2018_2/04/kaggle04_files/'\n",
    "DATA_PATH  = '/home/roma/Desktop/Tehnotrack_NeuralNetworks/4thHW/trainData/'\n",
    "train_path = 'homework_4.train.npy'\n",
    "test_path  = 'homework_4_no_classes.test.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_mean = np.mean([item[0].numpy() for item in CifarDataset(DATA_PATH + train_path, transform=transforms.ToTensor())], axis=(0,2,3))\n",
    "np_std = np.std([item[0].numpy() for item in CifarDataset(DATA_PATH + train_path, transform=transforms.ToTensor())], axis=(0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transform_norm = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.FloatTensor(np_mean), torch.FloatTensor(np_std))\n",
    "]\n",
    ")\n",
    "\n",
    "cifar_test_transform_norm = transforms.Compose([    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.FloatTensor(np_mean), torch.FloatTensor(np_std))\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_norm = CifarDataset(DATA_PATH + train_path, transform=cifar_transform_norm)\n",
    "dataloader_train_norm = DataLoader(dataset_train_norm, batch_size=128,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_test_norm = CifarDataset(DATA_PATH + test_path, is_train=False, transform=cifar_test_transform_norm)\n",
    "dataloader_test_norm = DataLoader(dataset_test_norm, batch_size=128,\n",
    "                        shuffle=False, num_workers=1)\n",
    "\n",
    "\n",
    "def train_network(a_net, \n",
    "                  a_device,\n",
    "                  dataloader_train_norm=dataloader_train_norm,\n",
    "                  a_epochs=164,\n",
    "                  a_batch_size=128,\n",
    "                  a_lr=0.1):\n",
    "    \n",
    "    train_acc = []\n",
    "    net = a_net.to(a_device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(a_net.parameters(), lr=a_lr, weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "    \n",
    "    for epoch in range(a_epochs):  # loop over the dataset multiple times\n",
    "        if epoch == 82:\n",
    "            optimizer = torch.optim.SGD(a_net.parameters(), lr=a_lr/10, weight_decay=0.0001, momentum=0.9) \n",
    "        elif epoch == 123:\n",
    "            optimizer = torch.optim.SGD(a_net.parameters(), lr=a_lr/100, weight_decay=0.0001, momentum=0.9) \n",
    "        \n",
    "        net = net.train()        \n",
    "        epoch_accuracy = 0.0\n",
    "        epoch_iters = 0\n",
    "        for item in dataloader_train_norm:\n",
    "            \n",
    "            epoch_iters += 1\n",
    "\n",
    "            inputs = item[0].to(a_device)\n",
    "            labels = item[1].long().to(a_device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_accuracy += accuracy_score(labels, prediction2classes(outputs))\n",
    "\n",
    "        epoch_accuracy /= epoch_iters\n",
    "        train_acc.append(epoch_accuracy)\n",
    "        \n",
    "        print(\"Epoch \", epoch, round(train_acc[-1], 4))\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    plt.plot(train_acc, label='Train')\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StupidDenseNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(StupidDenseNet, self).__init__()\n",
    "        \n",
    "        #Один из способов задать сеть - это задать последовательность слоев через Sequential\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('lin1', nn.Linear(3*32*32, 500))\n",
    "        self.classifier.add_module('sig1', torch.nn.Sigmoid())\n",
    "        self.classifier.add_module('lin2', nn.Linear(3*32*32, 500))\n",
    "        self.classifier.add_module('sig2', torch.nn.Sigmoid())\n",
    "        self.classifier.add_module('lin3', nn.Linear(3*32*32, 500))\n",
    "        self.classifier.add_module('sig3', torch.nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        ### Другой способ задания сети - это описать слои и в forward их применять явно\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        #Увеличиваем кол-во выходных слоев с 84 - до 84*2 - потому что классов 100\n",
    "        self.fc2 = nn.Linear(120, 84*2)\n",
    "        self.fc3 = nn.Linear(84*2, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CifarResidualBlock(nn.Module):\\n    def __init__(self, a_in_channels, make_downsample=False, use_skip_connection=True):\\n        super(CifarResidualBlock, self).__init__()\\n        self.use_skip_connection = use_skip_connection\\n        \\n        if make_downsample: coef = DOWNSAMPLE_COEF\\n        else: coef = 1  \\n            \\n        ### TODO - нужно описать используемые блоки\\n        pass\\n            \\n    def forward(self, x):\\n        ###TODO - описать forward блок с учетом флагов make_downsample и use_skip_connection\\n        return x\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class CifarResidualBlock(nn.Module):\n",
    "    def __init__(self, a_in_channels, make_downsample=False, use_skip_connection=True):\n",
    "        super(CifarResidualBlock, self).__init__()\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        \n",
    "        if make_downsample: coef = DOWNSAMPLE_COEF\n",
    "        else: coef = 1  \n",
    "            \n",
    "        ### TODO - нужно описать используемые блоки\n",
    "        pass\n",
    "            \n",
    "    def forward(self, x):\n",
    "        ###TODO - описать forward блок с учетом флагов make_downsample и use_skip_connection\n",
    "        return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSAMPLE_COEF = 2\n",
    "\n",
    "def conv3x3(a_in_planes, a_out_planes, a_stride=1):\n",
    "    \"\"\"\n",
    "    Основной строительный блок конволюций для ResNet\n",
    "    Включает в себя padding=1 - чтобы размерность сохранялась после его применения\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(a_in_planes, a_out_planes,  stride=a_stride,\n",
    "                     kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "def x_downsample(a_in_channels):\n",
    "     return nn.Conv2d(a_in_channels, \n",
    "               a_in_channels*DOWNSAMPLE_COEF,\n",
    "               kernel_size=1,\n",
    "               stride=2,\n",
    "               bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarResidualBlock(nn.Module):\n",
    "    def __init__(self, a_in_channels, make_downsample=False, use_skip_connection=True):\n",
    "        super(CifarResidualBlock, self).__init__()\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.make_downsample     = make_downsample\n",
    "        \n",
    "        if make_downsample: coef = DOWNSAMPLE_COEF\n",
    "        else: coef = 1  \n",
    "            \n",
    "        ### TODO - нужно описать используемые блоки\n",
    "        \n",
    "        self.DNSPL = x_downsample(a_in_channels)\n",
    "        \n",
    "        #self.CONV1 = conv3x3 (a_in_channels, a_in_channels * coef)\n",
    "        #self.RELU  = nn.ReLU ()\n",
    "        #self.CONV2 = conv3x3 (a_in_channels * coef, a_in_channels * coef, coef)\n",
    "        \n",
    "        \n",
    "        self.CONV1 = nn.Conv2d(    a_in_channels,\n",
    "                                   a_in_channels * coef,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1*coef,\n",
    "                                   padding=1,\n",
    "                                   bias=False)\n",
    "        \n",
    "        self.BNORM1= nn.BatchNorm2d(a_in_channels * coef, eps=1e-05, momentum=0.1, affine=False)\n",
    "        self.RELU  = nn.ReLU ()\n",
    "        \n",
    "        self.CONV2 = nn.Conv2d(    a_in_channels * coef,\n",
    "                                   a_in_channels * coef,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1,\n",
    "                                   bias=False)\n",
    "         \n",
    "        \n",
    "        self.BNORM2= nn.BatchNorm2d(a_in_channels * coef, eps=1e-05, momentum=0.1, affine=False)\n",
    "    def forward(self, x):\n",
    "        ###TODO - описать forward блок с учетом флагов make_downsample и use_skip_connection\n",
    "        \n",
    "        if self.use_skip_connection == True:\n",
    "            if self.make_downsample == True:\n",
    "                xprev = self.DNSPL(x)\n",
    "            else:\n",
    "                xprev = x\n",
    "        \n",
    "        #print ('sizeof(x) = ', x.size())\n",
    "        #print ('sizeof(xprev) = ', xprev.size())    \n",
    "        \n",
    "        x = self.CONV1  (x)\n",
    "        x = self.BNORM1 (x)\n",
    "        x = self.RELU   (x)\n",
    "        \n",
    "        x = self.CONV2  (x)\n",
    "        x = self.BNORM2 (x)\n",
    "        x = self.RELU   (x)\n",
    "        \n",
    "        if self.use_skip_connection == True:\n",
    "            x = xprev + x\n",
    "\n",
    "        x = self.RELU   (x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CifarResNet, self).__init__()\n",
    "        \n",
    "        #TODO нужно добавить блоков resnet и других слоев при необходимости\n",
    "        \n",
    "        self.features = nn.Sequential()\n",
    "        \n",
    "        #...\n",
    "        self.features.add_module('firstConv7x7', nn.Conv2d(3, 16, 7, padding=3))\n",
    "        self.features.add_module('res_block1_1', CifarResidualBlock(16))\n",
    "        self.features.add_module('res_block1_2', CifarResidualBlock(16))\n",
    "        self.features.add_module('res_block1_3', CifarResidualBlock(16))\n",
    "        self.features.add_module('res_DS_1'    , CifarResidualBlock(16, make_downsample=True))\n",
    "        self.features.add_module('res_block2_1', CifarResidualBlock(32))\n",
    "        self.features.add_module('res_block2_2', CifarResidualBlock(32))\n",
    "        self.features.add_module('res_block2_3', CifarResidualBlock(32))\n",
    "        self.features.add_module('res_DS_2'    , CifarResidualBlock(32, make_downsample=True))\n",
    "        self.features.add_module('res_block3_1', CifarResidualBlock(64))\n",
    "        self.features.add_module('res_block3_2', CifarResidualBlock(64))\n",
    "        self.features.add_module('res_block3_3', CifarResidualBlock(64))\n",
    "        self.features.add_module('res_DS_3'    , CifarResidualBlock(64, make_downsample=True))\n",
    "        self.features.add_module('res_block4_1', CifarResidualBlock(128))\n",
    "        self.features.add_module('res_block4_2', CifarResidualBlock(128))\n",
    "        self.features.add_module('res_block4_3', CifarResidualBlock(128))\n",
    "        #4x4\n",
    "        #...\n",
    "        \n",
    "        self.global_avg_pooling = nn.AvgPool2d(kernel_size=4)\n",
    "        self.fc_classifier = nn.Linear(128, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_avg_pooling(x)        \n",
    "        x = x.view((x.size()[0], -1))        \n",
    "        x = self.fc_classifier(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_net = StupidDenseNet()\n",
    "#%time train_network(dense_net, torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenet = LeNet()\n",
    "#lenet.cuda()\n",
    "#%time train_network(lenet, torch.device('cpu'), a_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenet = LeNet()\n",
    "#%time train_network(lenet, torch.device(DEVICE), a_epochs=10)\n",
    "#%time train_network(lenet, a_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4b2ad2085886>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(a_net, a_device, dataloader_train_norm, a_epochs, a_batch_size, a_lr)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "lenet = LeNet()\n",
    "%time train_network(lenet, torch.device(DEVICE), a_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 0.009\n",
      "Epoch  1 0.0091\n",
      "Epoch  2 0.009\n",
      "Epoch  3 0.0083\n",
      "Epoch  4 0.0095\n",
      "Epoch  5 0.0087\n",
      "Epoch  6 0.0097\n",
      "Epoch  7 0.0095\n",
      "Epoch  8 0.0087\n",
      "Epoch  9 0.0088\n",
      "Epoch  10 0.0092\n",
      "Epoch  11 0.0095\n",
      "Epoch  12 0.0093\n",
      "Epoch  13 0.0095\n",
      "Epoch  14 0.009\n",
      "Epoch  15 0.0091\n",
      "Epoch  16 0.0087\n",
      "Epoch  17 0.0099\n",
      "Epoch  18 0.0097\n",
      "Epoch  19 0.0095\n",
      "Epoch  20 0.0091\n",
      "Epoch  21 0.0086\n",
      "Epoch  22 0.0091\n",
      "Epoch  23 0.0089\n",
      "Epoch  24 0.0092\n",
      "Epoch  25 0.0094\n",
      "Epoch  26 0.0096\n",
      "Epoch  27 0.0098\n",
      "Epoch  28 0.0092\n",
      "Epoch  29 0.0095\n",
      "Epoch  30 0.0088\n",
      "Epoch  31 0.0096\n",
      "Epoch  32 0.0088\n",
      "Epoch  33 0.009\n",
      "Epoch  34 0.0099\n",
      "Epoch  35 0.0092\n",
      "Epoch  36 0.0088\n",
      "Epoch  37 0.0101\n",
      "Epoch  38 0.0091\n",
      "Epoch  39 0.0085\n",
      "Epoch  40 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-165:\n",
      "Process Process-166:\n",
      "Process Process-168:\n",
      "Process Process-167:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/roma/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4b2ad2085886>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(a_net, a_device, dataloader_train_norm, a_epochs, a_batch_size, a_lr)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction2classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mepoch_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0d4b84f9050f>\u001b[0m in \u001b[0;36mprediction2classes\u001b[0;34m(output_var)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet = CifarResNet()\n",
    "%time train_network(resnet, torch.device(DEVICE), a_epochs = 50, a_lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Важно переключить сеть в режим eval - иначе dropout будет работать некорректно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_solution(a_net, a_device):\n",
    "    res = []\n",
    "    net = a_net.eval()\n",
    "    for item in dataloader_test_norm:\n",
    "        inputs = item.to(a_device)\n",
    "        outputs = net(inputs) \n",
    "\n",
    "        res += prediction2classes(outputs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_solution = make_solution(dense_net, DEVICE)\n",
    "my_solution = make_solution(lenet, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_solution.csv', 'w') as fout:\n",
    "    print('Id', 'Prediction', sep=',', file=fout)\n",
    "    for i, prediction in enumerate(my_solution):\n",
    "        print(i, prediction, sep=',', file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
