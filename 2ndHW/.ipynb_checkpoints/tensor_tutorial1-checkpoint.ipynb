{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda install pytorch torchvision -c soumith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors behave almost exactly the same way in PyTorch as they do in\n",
    "Torch.\n",
    "\n",
    "Create a tensor of size (5 x 7) with uninitialized memory:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a tensor randomized with a normal distribution with mean=0, var=1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.9326  1.5570 -0.2717  0.4389  1.4863 -0.1356 -0.0794\n",
      " 0.4314  0.3124  0.5090 -0.1026  0.4481 -0.2089  0.3813\n",
      " 1.8349 -1.0073  1.6821 -1.3503  1.7002 -0.3905 -1.4316\n",
      " 1.8844 -0.6183  1.7707  0.7444  1.1600  1.1696  3.2074\n",
      " 2.1823  0.3568  1.4908  2.7820  0.4235 -1.3837  2.4151\n",
      "[torch.FloatTensor of size 5x7]\n",
      "\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports the same operations</p></div>\n",
    "\n",
    "Inplace / Out-of-place\n",
    "----------------------\n",
    "\n",
    "The first difference is that ALL operations on the tensor that operate\n",
    "in-place on it will have an ``_`` postfix. For example, ``add`` is the\n",
    "out-of-place version, and ``add_`` is the in-place version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000\n",
      " 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000\n",
      " 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000\n",
      " 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000\n",
      " 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000\n",
      "[torch.FloatTensor of size 5x7]\n",
      " \n",
      " 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000\n",
      " 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000\n",
      " 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000\n",
      " 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000\n",
      " 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000\n",
      "[torch.FloatTensor of size 5x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "# a has now been filled with the value 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a is still filled with 3.5\n",
    "# new tensor b is returned with values 3.5 + 4.0 = 7.5\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations like ``narrow`` do not have in-place versions, and\n",
    "hence, ``.narrow_`` does not exist. Similarly, some operations like\n",
    "``fill_`` do not have an out-of-place version, so ``.fill`` does not\n",
    "exist.\n",
    "\n",
    "Zero Indexing\n",
    "-------------\n",
    "\n",
    "Another difference is that Tensors are zero-indexed. (In lua, tensors are\n",
    "one-indexed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0, 3]  # select 1st row, 4th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be also indexed with Python's slicing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, 3:5]  # selects all rows, 4th column and  5th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No camel casing\n",
    "---------------\n",
    "\n",
    "The next small difference is that all functions are now NOT camelCase\n",
    "anymore. For example ``indexAdd`` is now called ``index_add_``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.LongTensor([4, 0]), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Bridge\n",
    "------------\n",
    "\n",
    "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
    "The torch Tensor and numpy array will share their underlying memory\n",
    "locations, and changing one will change the other.\n",
    "\n",
    "Converting torch Tensor to numpy Array\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) \t# see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy Array to torch Tensor\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to\n",
    "NumPy and back.\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "CUDA Tensors are nice and easy in pytorch, and transfering a CUDA tensor\n",
    "from the CPU to GPU will retain its underlying type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # creates a LongTensor and transfers it\n",
    "    # to GPU as torch.cuda.LongTensor\n",
    "    a = torch.LongTensor(10).fill_(3).cuda()\n",
    "    print(type(a))\n",
    "    b = a.cpu()\n",
    "    # transfers it to CPU, back to\n",
    "    # being a torch.LongTensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
