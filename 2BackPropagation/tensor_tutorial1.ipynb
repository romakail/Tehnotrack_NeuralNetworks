{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#conda install pytorch torchvision -c soumith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors behave almost exactly the same way in PyTorch as they do in\n",
    "Torch.\n",
    "\n",
    "Create a tensor of size (5 x 7) with uninitialized memory:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7775e-17, 3.0854e-41, 1.3095e+10, 7.2053e+22, 4.7428e+30, 3.0881e+29,\n",
      "         1.8499e+20],\n",
      "        [5.0830e+31, 2.9588e+21, 7.1558e+22, 1.6045e+02, 1.6370e-19, 1.8617e+25,\n",
      "         1.8936e+23],\n",
      "        [1.6045e+02, 1.6370e-19, 1.8888e+31, 5.0848e+31, 1.8178e+31, 1.3556e-19,\n",
      "         7.1463e+22],\n",
      "        [4.6241e+30, 1.0552e+24, 1.9345e-19, 7.4086e+28, 4.9571e+28, 7.2251e+28,\n",
      "         5.2487e-14],\n",
      "        [2.3745e+23, 7.7764e+31, 2.7947e+20, 4.5925e+24, 1.7448e+22, 4.2325e+21,\n",
      "         9.2246e-39]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(5, 7)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a tensor randomized with a normal distribution with mean=0, var=1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0143,  1.5962,  0.6341,  0.9280, -2.6694,  1.2878,  1.6346],\n",
      "        [-2.5934,  0.5811, -1.6028,  0.1085, -0.1649, -1.4704, -0.3940],\n",
      "        [-0.7154, -1.0605,  2.0713,  0.8790, -0.4428,  0.0108, -1.6543],\n",
      "        [ 4.2511, -2.0982,  0.6184,  1.5163,  0.1774, -1.7513,  0.3362],\n",
      "        [ 0.2034, -1.2948,  0.5766,  1.0218, -0.4834, -1.3211,  2.3492]])\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports the same operations</p></div>\n",
    "\n",
    "Inplace / Out-of-place\n",
    "----------------------\n",
    "\n",
    "The first difference is that ALL operations on the tensor that operate\n",
    "in-place on it will have an ``_`` postfix. For example, ``add`` is the\n",
    "out-of-place version, and ``add_`` is the in-place version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]) tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]])\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "# a has now been filled with the value 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a is still filled with 3.5\n",
    "# new tensor b is returned with values 3.5 + 4.0 = 7.5\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations like ``narrow`` do not have in-place versions, and\n",
    "hence, ``.narrow_`` does not exist. Similarly, some operations like\n",
    "``fill_`` do not have an out-of-place version, so ``.fill`` does not\n",
    "exist.\n",
    "\n",
    "Zero Indexing\n",
    "-------------\n",
    "\n",
    "Another difference is that Tensors are zero-indexed. (In lua, tensors are\n",
    "one-indexed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5000)\n"
     ]
    }
   ],
   "source": [
    "b = a[0, 3]  # select 1st row, 4th column from a\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be also indexed with Python's slicing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "b = a[:, 3:5]  # selects all rows, 4th column and  5th column from a\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No camel casing\n",
    "---------------\n",
    "\n",
    "The next small difference is that all functions are now NOT camelCase\n",
    "anymore. For example ``indexAdd`` is now called ``index_add_``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.LongTensor([4, 0]), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Bridge\n",
    "------------\n",
    "\n",
    "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
    "The torch Tensor and numpy array will share their underlying memory\n",
    "locations, and changing one will change the other.\n",
    "\n",
    "Converting torch Tensor to numpy Array\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\t# see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy Array to torch Tensor\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to\n",
    "NumPy and back.\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "CUDA Tensors are nice and easy in pytorch, and transfering a CUDA tensor\n",
    "from the CPU to GPU will retain its underlying type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # creates a LongTensor and transfers it\n",
    "    # to GPU as torch.cuda.LongTensor\n",
    "    a = torch.LongTensor(10).fill_(3).cuda()\n",
    "    print(type(a))\n",
    "    b = a.cpu()\n",
    "    # transfers it to CPU, back to\n",
    "    # being a torch.LongTensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
